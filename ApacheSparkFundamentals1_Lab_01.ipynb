{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded!\n"
     ]
    }
   ],
   "source": [
    "# download the data from the IBM server\n",
    "# this may take ~30 seconds depending on your internet speed\n",
    "!wget --quiet https://cocl.us/BD0211EN_Data\n",
    "print(\"Data Downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted!\n"
     ]
    }
   ],
   "source": [
    "# this may take ~30 seconds depending on your internet speed\n",
    "!unzip -q -o -d /resources/jupyterlab/labs/BD0211EN/ BD0211EN_Data\n",
    "print(\"Data Extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "followers.txt\n",
      "notebook.log\n",
      "nyctaxi.csv\n",
      "nyctaxi100.csv\n",
      "nyctaxisub.csv\n",
      "nycweather.csv\n",
      "pom.xml\n",
      "taxistreams.py\n",
      "users.txt\n"
     ]
    }
   ],
   "source": [
    "# list the extracted files\n",
    "!ls -1 /resources/jupyterlab/labs/BD0211EN/LabData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j--src.zip:%PYTHONPATH%\n",
      "Requirement already satisfied: findspark in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (1.4.2)\n",
      "Collecting pyspark==2.4.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 81kB/s  eta 0:00:013    |██▋                             | 17.6MB 44.5MB/s eta 0:00:05                         | 27.6MB 44.5MB/s eta 0:00:05 |██████                          | 39.8MB 39.0MB/s eta 0:00:05     |██████▏                         | 41.3MB 5.2MB/s eta 0:00:34     |████████▏                       | 55.4MB 38.7MB/s eta 0:00:05��█████████▊                  | 92.8MB 45.6MB/s eta 0:00:03     |██████████████                  | 94.4MB 45.6MB/s eta 0:00:03�█████████████▏                 | 95.2MB 45.6MB/s eta 0:00:03     |█████████████████               | 114.4MB 41.8MB/s eta 0:00:03     |██████████████████▎             | 123.4MB 30.9MB/s eta 0:00:03     |███████████████████             | 127.5MB 30.9MB/s eta 0:00:03MB/s eta 0:00:03MB/s eta 0:00:02��█████████████████▎      | 170.5MB 5.5MB/s eta 0:00:09   | 184.1MB 5.5MB/s eta 0:00:06     |██████████████████████████████  | 202.5MB 7.1MB/s eta 0:00:02     |██████████████████████████████▋ | 206.0MB 7.1MB/s eta 0:00:02    |██████████████████████████████▉ | 207.5MB 7.1MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.4)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Found existing installation: py4j 0.10.9\n",
      "    Uninstalling py4j-0.10.9:\n",
      "      Successfully uninstalled py4j-0.10.9\n",
      "  Found existing installation: pyspark 3.0.0\n",
      "    Uninstalling pyspark-3.0.0:\n",
      "      Successfully uninstalled pyspark-3.0.0\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-10c3590fd669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mSparkContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "%env PYTHONPATH=%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j--src.zip:%PYTHONPATH%\n",
    "\n",
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "\n",
    "!pip install pyspark==2.4.4\n",
    "\n",
    "import pyspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "SparkContext = sc\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j--src.zip:%PYTHONPATH%\n",
      "Requirement already satisfied: findspark in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyspark==2.4.4 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from pyspark==2.4.4) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONPATH=%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j--src.zip:%PYTHONPATH%\n",
    "\n",
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "\n",
    "!pip install pyspark==2.4.4\n",
    "\n",
    "import pyspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "sc.version\n",
    "\n",
    "SparkContext = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithSpark = readme.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark = readme.filter(lambda line: \"Spark\" in line)\n",
    "readme.filter(lambda line: \"Spark\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max(a, b):\n",
    " if a > b:\n",
    "    return a\n",
    " else:\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = readme.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 14),\n",
       " ('is', 6),\n",
       " ('It', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 5),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('an', 3),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('supports', 2),\n",
       " ('computation', 1),\n",
       " ('analysis.', 1),\n",
       " ('set', 2),\n",
       " ('of', 5),\n",
       " ('tools', 1),\n",
       " ('SQL', 2),\n",
       " ('MLlib', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('Documentation', 1),\n",
       " ('latest', 1),\n",
       " ('programming', 1),\n",
       " ('guide,', 1),\n",
       " ('[project', 2),\n",
       " ('README', 1),\n",
       " ('only', 1),\n",
       " ('basic', 1),\n",
       " ('instructions.', 1),\n",
       " ('Building', 1),\n",
       " ('using', 2),\n",
       " ('[Apache', 1),\n",
       " ('run:', 1),\n",
       " ('do', 2),\n",
       " ('this', 1),\n",
       " ('downloaded', 1),\n",
       " ('documentation', 3),\n",
       " ('project', 1),\n",
       " ('site,', 1),\n",
       " ('at', 2),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('Interactive', 2),\n",
       " ('Shell', 2),\n",
       " ('The', 1),\n",
       " ('way', 1),\n",
       " ('start', 1),\n",
       " ('Try', 1),\n",
       " ('following', 2),\n",
       " ('1000:', 2),\n",
       " ('scala>', 1),\n",
       " ('1000).count()', 1),\n",
       " ('Python', 2),\n",
       " ('Alternatively,', 1),\n",
       " ('use', 3),\n",
       " ('And', 1),\n",
       " ('run', 7),\n",
       " ('Example', 1),\n",
       " ('several', 1),\n",
       " ('programs', 2),\n",
       " ('them,', 1),\n",
       " ('`./bin/run-example', 1),\n",
       " ('[params]`.', 1),\n",
       " ('example:', 1),\n",
       " ('./bin/run-example', 2),\n",
       " ('SparkPi', 2),\n",
       " ('variable', 1),\n",
       " ('when', 1),\n",
       " ('examples', 2),\n",
       " ('spark://', 1),\n",
       " ('URL,', 1),\n",
       " ('YARN,', 1),\n",
       " ('\"local\"', 1),\n",
       " ('locally', 2),\n",
       " ('N', 1),\n",
       " ('abbreviated', 1),\n",
       " ('class', 2),\n",
       " ('name', 1),\n",
       " ('package.', 1),\n",
       " ('instance:', 1),\n",
       " ('print', 1),\n",
       " ('usage', 1),\n",
       " ('help', 1),\n",
       " ('no', 1),\n",
       " ('params', 1),\n",
       " ('are', 1),\n",
       " ('Testing', 1),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Once', 1),\n",
       " ('built,', 1),\n",
       " ('tests', 2),\n",
       " ('using:', 1),\n",
       " ('./dev/run-tests', 1),\n",
       " ('Please', 3),\n",
       " ('guidance', 3),\n",
       " ('module,', 1),\n",
       " ('individual', 1),\n",
       " ('Note', 1),\n",
       " ('About', 1),\n",
       " ('uses', 1),\n",
       " ('library', 1),\n",
       " ('HDFS', 1),\n",
       " ('other', 1),\n",
       " ('Hadoop-supported', 1),\n",
       " ('storage', 1),\n",
       " ('systems.', 1),\n",
       " ('Because', 1),\n",
       " ('have', 1),\n",
       " ('changed', 1),\n",
       " ('different', 1),\n",
       " ('versions', 1),\n",
       " ('Hadoop,', 2),\n",
       " ('must', 1),\n",
       " ('against', 1),\n",
       " ('version', 1),\n",
       " ('refer', 2),\n",
       " ('particular', 3),\n",
       " ('distribution', 1),\n",
       " ('Hive', 2),\n",
       " ('Thriftserver', 1),\n",
       " ('distributions.', 1),\n",
       " ('[\"Third', 1),\n",
       " ('distribution.', 1),\n",
       " ('[Configuration', 1),\n",
       " ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " ('online', 1),\n",
       " ('overview', 1),\n",
       " ('configure', 1),\n",
       " ('Spark.', 1),\n",
       " ('a', 10),\n",
       " ('fast', 1),\n",
       " ('and', 10),\n",
       " ('general', 2),\n",
       " ('cluster', 2),\n",
       " ('computing', 1),\n",
       " ('system', 1),\n",
       " ('for', 12),\n",
       " ('Big', 1),\n",
       " ('Data.', 1),\n",
       " ('Python,', 2),\n",
       " ('R,', 1),\n",
       " ('that', 3),\n",
       " ('graphs', 1),\n",
       " ('data', 1),\n",
       " ('also', 5),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 3),\n",
       " ('DataFrames,', 1),\n",
       " ('Streaming', 1),\n",
       " ('stream', 1),\n",
       " ('processing.', 1),\n",
       " ('<http://spark.apache.org/>', 1),\n",
       " ('##', 8),\n",
       " ('Online', 1),\n",
       " ('You', 3),\n",
       " ('can', 6),\n",
       " ('find', 1),\n",
       " ('the', 21),\n",
       " ('documentation,', 1),\n",
       " ('on', 6),\n",
       " ('web', 1),\n",
       " ('page](http://spark.apache.org/documentation.html)', 1),\n",
       " ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1),\n",
       " ('This', 2),\n",
       " ('file', 1),\n",
       " ('contains', 1),\n",
       " ('setup', 1),\n",
       " ('built', 1),\n",
       " ('Maven](http://maven.apache.org/).', 1),\n",
       " ('To', 2),\n",
       " ('build', 3),\n",
       " ('its', 1),\n",
       " ('example', 3),\n",
       " ('programs,', 1),\n",
       " ('build/mvn', 1),\n",
       " ('-DskipTests', 1),\n",
       " ('clean', 1),\n",
       " ('package', 1),\n",
       " ('(You', 1),\n",
       " ('not', 1),\n",
       " ('need', 1),\n",
       " ('to', 14),\n",
       " ('if', 4),\n",
       " ('you', 4),\n",
       " ('pre-built', 1),\n",
       " ('package.)', 1),\n",
       " ('More', 1),\n",
       " ('detailed', 2),\n",
       " ('available', 1),\n",
       " ('from', 1),\n",
       " ('[\"Building', 1),\n",
       " ('Scala', 2),\n",
       " ('easiest', 1),\n",
       " ('through', 1),\n",
       " ('shell:', 2),\n",
       " ('./bin/spark-shell', 1),\n",
       " ('command,', 2),\n",
       " ('which', 2),\n",
       " ('should', 2),\n",
       " ('return', 2),\n",
       " ('sc.parallelize(1', 1),\n",
       " ('prefer', 1),\n",
       " ('./bin/pyspark', 1),\n",
       " ('>>>', 1),\n",
       " ('sc.parallelize(range(1000)).count()', 1),\n",
       " ('Programs', 1),\n",
       " ('comes', 1),\n",
       " ('with', 4),\n",
       " ('sample', 1),\n",
       " ('`examples`', 2),\n",
       " ('directory.', 1),\n",
       " ('one', 2),\n",
       " ('<class>', 1),\n",
       " ('For', 2),\n",
       " ('will', 1),\n",
       " ('Pi', 1),\n",
       " ('locally.', 1),\n",
       " ('MASTER', 1),\n",
       " ('environment', 1),\n",
       " ('running', 1),\n",
       " ('submit', 1),\n",
       " ('cluster.', 1),\n",
       " ('be', 2),\n",
       " ('mesos://', 1),\n",
       " ('or', 3),\n",
       " ('\"yarn\"', 1),\n",
       " ('thread,', 1),\n",
       " ('\"local[N]\"', 1),\n",
       " ('threads.', 1),\n",
       " ('MASTER=spark://host:7077', 1),\n",
       " ('Many', 1),\n",
       " ('given.', 1),\n",
       " ('Running', 1),\n",
       " ('Tests', 1),\n",
       " ('first', 1),\n",
       " ('requires', 1),\n",
       " ('[building', 1),\n",
       " ('see', 1),\n",
       " ('how', 2),\n",
       " ('[run', 1),\n",
       " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       "  1),\n",
       " ('A', 1),\n",
       " ('Hadoop', 4),\n",
       " ('Versions', 1),\n",
       " ('core', 1),\n",
       " ('talk', 1),\n",
       " ('protocols', 1),\n",
       " ('same', 1),\n",
       " ('your', 1),\n",
       " ('runs.', 1),\n",
       " ('[\"Specifying', 1),\n",
       " ('Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       "  1),\n",
       " ('building', 3),\n",
       " ('See', 1),\n",
       " ('Party', 1),\n",
       " ('Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)',\n",
       "  1),\n",
       " ('application', 1),\n",
       " ('works', 1),\n",
       " ('Configuration', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(linesWithSpark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import Timer\n",
    "def count():\n",
    "    return linesWithSpark.count()\n",
    "t = Timer(lambda: count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.527039118111134\n"
     ]
    }
   ],
   "source": [
    "print(t.timeit(number=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.937123050913215\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark.cache()\n",
    "print(t.timeit(number=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
